{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b00932",
   "metadata": {},
   "source": [
    "# 0 Libraries and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe0190",
   "metadata": {},
   "source": [
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c717c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math as m\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from matplotlib.ticker import ScalarFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897ff18",
   "metadata": {},
   "source": [
    "Set the background color of figures to white as default when saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['savefig.facecolor']='white'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a00003",
   "metadata": {},
   "source": [
    "# 1 Sampling from large particle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0796d",
   "metadata": {},
   "source": [
    "## 1.1 Read in and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb1324",
   "metadata": {},
   "source": [
    "Read in the particle data stored as csv file (e.g. as output of ImageJ-ParticleSizer analysis) and save them as Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c310f",
   "metadata": {},
   "source": [
    "In case of a single csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv('ParticleSizer_data.csv')\n",
    "print (frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46e738",
   "metadata": {},
   "source": [
    "In case you want to combine data from multiple csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for csvFilename in os.listdir(os.getcwd()):\n",
    "    if not csvFilename.endswith('.csv'):\n",
    "        continue\n",
    "    df = pd.read_csv(csvFilename)\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_)\n",
    "print (frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054f6cb",
   "metadata": {},
   "source": [
    "Let's clean the dataframe by deleting rows which contain unreadable/uninterpretable data, i.e. NaNs (not a number) or infinity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6dbf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "frame = frame.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86b2a2",
   "metadata": {},
   "source": [
    "The dataframe contains many columns with data for a variety of particle parameters that were outputted by the ParticleSizer software. In this example, we are only interested in the data of the following parameters: minimum and maximum Feret diameter (Fmin, Fmax), area equivalent circle diameter (ECD), maximum inscribed circle diameter (MICD) and aspect ratio (AR). Unify the column names of interest and create a new dataframe keeping only those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3685a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.rename(columns={'Min. Feret': 'Fmin'}, inplace=True)\n",
    "frame.rename(columns={'Feret': 'Fmax'}, inplace=True)\n",
    "frame.rename(columns={'Area equivalent circle diameter': 'ECD'}, inplace=True)\n",
    "frame.rename(columns={'Maximum inscribed circle diameter': 'MICD'}, inplace=True)\n",
    "frame.rename(columns={'Aspect Ratio': 'AR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Fmin', 'Fmax', 'ECD', 'MICD', 'AR']  \n",
    "frame=frame[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b06152",
   "metadata": {},
   "source": [
    "We want to output the data in unit of nanometer. In case the data was in a different unit (e.g. micrometer) convert the columns of interest to nanometer (e.g. x1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca24c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['Fmin'] = frame['Fmin'].multiply(1000)\n",
    "frame['Fmax'] = frame['Fmax'].multiply(1000)\n",
    "frame['ECD'] = frame['ECD'].multiply(1000)\n",
    "frame['MICD'] = frame['MICD'].multiply(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4ff84",
   "metadata": {},
   "source": [
    "## 1.2 Full dataset statistics + histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca94502",
   "metadata": {},
   "source": [
    "Calculate the statistics of the Fmin, Fmax, ECD, MICD and AR measurands using the pandas describe function. Include also the calculation of the interquartile range (IQR) and the normalized IQR (IQR% = IQR/D50) and write them to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76204a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df, output_file):\n",
    "    # Specify the percentiles to be calculated\n",
    "    percentiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "    \n",
    "    # Use describe to get statistics including specified percentiles\n",
    "    desc = df.describe(percentiles=percentiles).T\n",
    "    \n",
    "    # Calculate IQR (Interquartile Range)\n",
    "    desc['IQR'] = desc['75%'] - desc['25%']\n",
    "    \n",
    "    # Calculate IQR%\n",
    "    desc['IQR%'] = desc['IQR'] / desc['50%'] * 100\n",
    "    \n",
    "    # Re-arrange the DataFrame to include IQR and IQR% in the output\n",
    "    stats_df = desc[['count', 'mean', 'std', 'min', '10%', '25%', '50%', '75%', '90%', 'max', 'IQR', 'IQR%']]\n",
    "    \n",
    "    # Transpose so that statistical measures are rows and columns are original DataFrame columns\n",
    "    stats_df = stats_df.T\n",
    "    \n",
    "    # Write the result to a CSV file\n",
    "    stats_df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_statistics(frame, 'Full_set_statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750865a4",
   "metadata": {},
   "source": [
    "Set the measurand + unit you want to plot the histogram for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurand = 'Fmin'\n",
    "unit = '(nm)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd22118",
   "metadata": {},
   "source": [
    "Define the function to plot the histogram. The function contains a boolean 'PercentileLines' to determine whether vertical lines at the positions of the D10, D25, D50, D75, D90 should be drawn on the histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d97219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistogram(df, PercentileLines: bool):\n",
    "    \n",
    "    hist = sns.histplot(df, label='N = ' + str(len(df.index)), bins = 'fd') # bins according to Freedman-Diaconis rule\n",
    "    \n",
    "    # if PercentileLines is True, the next few lines of code will plot vertical lines on the histogram at the position of the different percentiles\n",
    "    if PercentileLines:\n",
    "        perc_txt = ['D10', 'D25', 'D50', 'D75', 'D90']\n",
    "        percs = [df.quantile(0.1), df.quantile(0.25), df.quantile(0.5), df.quantile(0.75), df.quantile(0.9)]\n",
    "        max_y_value = max([p.get_height() for p in hist.patches])\n",
    "        for i, p in enumerate(percs):\n",
    "            plt.axvline(x = p, color = 'k', linestyle = '--')\n",
    "            plt.text(p - (percs[0]/10), max_y_value - (max_y_value/8), perc_txt[i], rotation = 'vertical', fontsize = 14)\n",
    "    \n",
    "    # plot settings       \n",
    "    plt.xlabel(measurand + ' ' + unit, fontsize = 20)\n",
    "    plt.ylabel('Count', fontsize=20)\n",
    "    #plt.xlim(0,35)\n",
    "    #plt.ylim(0,500)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save the histogram as a figure\n",
    "    plt.savefig(measurand + '_hist_N=' + str(len(df.index)) +'.png', dpi = 200)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c799d",
   "metadata": {},
   "source": [
    "Plot the histogram of Fmin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistogram(frame[measurand], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff78baf",
   "metadata": {},
   "source": [
    "## 1.3 Sampling subdatasets of increasing particle size N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ba711",
   "metadata": {},
   "source": [
    "Create a list of N values between 10 and the max of the full data set. For each of these values of N a subdataset will be sampled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77037fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of 80 integers logaritmically spaced between 10 and 10^4\n",
    "n=np.logspace(1, 4, num=80, dtype='int')\n",
    "\n",
    "# In case the size of the dataset (N_tot) is smaller than 10^4, N values which are larger N_tot are removed. The N_tot value is added as the max N for sampling.\n",
    "n=n[n< len(frame)]\n",
    "if(n[-1]<10000): n=np.append(n, len(frame))\n",
    "    \n",
    "# Delete repeating numbers\n",
    "n=np.unique(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71030d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c863e",
   "metadata": {},
   "source": [
    "Construct a function to create subsamples of measured particles of increasing size N for a chosen number of samples per N. Particles are drawn with replacement, meaning each particle can be drawn more than once. In this way, multiple samples up to size N_tot can be drawn without creating identical samples. The function returns a set of dataframes (a dataframe for each measurand) where for each N the mean, standard deviation (std) and U_N is calculated. U_N is defined as the relative std (=std/mean) at a confidence interval of 95% (k=2) and given as a percentage value: U_N = 2 * 100 * std/mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubSampling(df, numSamplesPerNumParticles: int):\n",
    "    \n",
    "    # create an empty numpy array of objects with size equal to the number of columns in the dataframe, i.e. the number of measurands\n",
    "    measurands = df.columns.values\n",
    "    print(measurands)\n",
    "    stats = np.empty(len(measurands), dtype=object)\n",
    "    \n",
    "    # loop over the N values\n",
    "    for i in tqdm(n):    # tqdm shows a status bar for the for loop in the console + time estimate\n",
    "        \n",
    "        # loop over the number of subsamples\n",
    "        for j in range(0, numSamplesPerNumParticles):\n",
    "            # take a sample of size N with replacement\n",
    "            df_s = df.sample(n=i, replace=True)\n",
    "            \n",
    "            #loop over the different measurands\n",
    "            for k,m in enumerate(measurands):\n",
    "                # calculate the statistics including percentiles of the subsample\n",
    "                stats_temp = df_s[m].describe(percentiles=[0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95])\n",
    "                \n",
    "                # Add the statistics calculated for each N and for each sample to a dataframe\n",
    "                stats[k] = pd.concat([stats[k], stats_temp.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    # calculate the dataframes grouped by \"count\"(=particle number N), where the mean, std and U_N (saved as <lambda_0>) of the different samples at each N are calculated and store them in a list \n",
    "    data_out = []\n",
    "    for s in range(0, len(stats)):\n",
    "        data_out.append(stats[s].groupby(\"count\").agg([np.mean, np.std, lambda x: 2*100* (np.std(x) / np.mean(x))]))\n",
    "        \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8120a27",
   "metadata": {},
   "source": [
    "Run the function for 500 subsamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SamplingData = SubSampling(frame, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40104c93",
   "metadata": {},
   "source": [
    "Define a function to rename the percentile columns in the sampling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenamePercCols(df):\n",
    "    df.rename(\n",
    "        columns={'5%': 'D5', '10%': 'D10', '25%': 'D25', '50%': 'D50', '75%': 'D75', '90%': 'D90', '95%': 'D95'},\n",
    "        inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730ef81",
   "metadata": {},
   "source": [
    "Apply the above generated function and save the dataframes as csv files for each measurand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fr in enumerate(SamplingData):\n",
    "    RenamePercCols(fr)\n",
    "    measurand = frame.columns[i]\n",
    "    fr.to_csv('SubSampling_Stats_'+measurand+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38981198",
   "metadata": {},
   "source": [
    "# 2 Plotting and fitting relation between N and U_N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a568a",
   "metadata": {},
   "source": [
    "The uncertainty related to particle number U_N is defined at each N by the relative std of the characteristic calculated based on the subsamples. It is expressed with a confidence interval of 95% (k=2). We will now plot the relation between N and U_N and determine the number of particles required to reach specific values of U_N by fitting the relation. Let's say we wanna do this for the Fmin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766bef4",
   "metadata": {},
   "source": [
    "First, let's read in the subsampling data produced in section 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72283b2b",
   "metadata": {},
   "source": [
    "Verify that you are in the right directory where the data is stored. If needed you can change the directory with the os.chdir() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "#os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9070e49",
   "metadata": {},
   "source": [
    "Read in the data as a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd422dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurand = 'Fmin' # choose Fmin, Fmax, ECD, MICD, AR or other\n",
    "data = pd.read_csv('SubSampling_Stats_'+measurand+'.csv', header=[0, 1], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae86c1c",
   "metadata": {},
   "source": [
    "Make a new folder, named according to the analyzed measurand, to save the generated output plots and change the directory to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42259a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = measurand + '_output'\n",
    "#os.makedirs(folder) # comment this line when the folder already exists\n",
    "os.chdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8d85d",
   "metadata": {},
   "source": [
    "## 2.1 Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc68399",
   "metadata": {},
   "source": [
    "Define a function that plots a specific column (defined by two column titles: column = D10, D50, etc..; par = mean, std, <lambda_0>) of a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_datacolumn(df, column: str, par: str):\n",
    "    \n",
    "    # depending on the parameter 'par', set the correct label and unit for the y-axis\n",
    "    if par == '<lambda_0>': \n",
    "        par_l = r'$U_{\\rm N}$'\n",
    "        unit = '(%)'\n",
    "    elif par == 'mean':\n",
    "        par_l = 'Mean'\n",
    "        unit = '(nm)'\n",
    "    elif par == 'std':\n",
    "        par_l = 'Std'\n",
    "        unit = '(nm)'\n",
    "    else:\n",
    "        par_l = par\n",
    "        unit = ''\n",
    "    \n",
    "    # plot the data on a semilog plot\n",
    "    plt.semilogx(df.index, df[column, par], '.', label=column)\n",
    "    \n",
    "    # plot settings\n",
    "    plt.ylabel(par_l + ' ' + unit, fontsize=20)      \n",
    "    plt.xlabel(r'$N$', fontsize=20)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlim(10,df.index[-1])\n",
    "    plt.legend(fontsize = 14)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce295e",
   "metadata": {},
   "source": [
    "Plot the mean and U_N as a function of N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datacolumn(data, 'D50', 'mean')\n",
    "plt.show()\n",
    "plot_datacolumn(data, 'D50', '<lambda_0>')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc90f5b",
   "metadata": {},
   "source": [
    "Plot U_N of all percentiles as a function of N on one plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc=['D10', 'D25', 'D50', 'D75', 'D90']\n",
    "for p in perc:\n",
    "    plot_datacolumn(data, p, '<lambda_0>')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a615f8d",
   "metadata": {},
   "source": [
    "Do the same but on a logscale for the y axis as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc=['D10', 'D25', 'D50', 'D75', 'D90']\n",
    "for p in perc:\n",
    "    plot_datacolumn(data, p, '<lambda_0>')\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.1,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb692f0",
   "metadata": {},
   "source": [
    "## 2.2 Fitting U_N using UnivariateSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd0c3cd",
   "metadata": {},
   "source": [
    "Set parameter for which you wanna do the fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "par = 'D50'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7770be",
   "metadata": {},
   "source": [
    "Create an array of x values to be used for the fitting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(1, 4, 1000)\n",
    "xs_f = 10**xs\n",
    "xs_f = xs_f[xs_f<=data.index[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ecc2a1",
   "metadata": {},
   "source": [
    "Define a function to fit a smooth curve to the U_N vs. N data, using the UnivariateSpline function. The smoothing parameter defines the degree of smooting and this will have to be adapted manually to achieve the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ecefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothCurve(df, column, smoothing):\n",
    "    # x data need to be converted to linear scale for the spline to work\n",
    "    xlogScale = np.log10(df.index)  \n",
    "    \n",
    "    # perform the fit using UnivariateSpline function\n",
    "    s = UnivariateSpline(xlogScale, df[column, '<lambda_0>'], s = smoothing) #s = smoothing factor, the higher the more smoothing is applied\n",
    "    ys = s(xs)\n",
    "    ys_f = ys[0:len(xs_f)]\n",
    "    \n",
    "    # plot the fit function and plot the data on the same figure\n",
    "    plt.plot(xs_f, ys_f, 'k-')\n",
    "    plot_datacolumn(df, column, '<lambda_0>')\n",
    "    \n",
    "    #plot settings\n",
    "    plt.xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #return the y values of the fit curve\n",
    "    return ys_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855787e0",
   "metadata": {},
   "source": [
    "Perform the fitting procedure for the D50 data and adapt the smoothing value until a nice smooth curve fit is obtained. Increasing the value will make the curve more smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = 3\n",
    "smoothCurve(data, 'D50', smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771df4f",
   "metadata": {},
   "source": [
    "The next lines of code will do the smoothing for each of the percentiles of the chosen measurand at the same time and save the output curves in the array sC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smo_list = [15, 8, 3, 0.7, 2]\n",
    "perc=['D10', 'D25', 'D50', 'D75', 'D90']\n",
    "sC = [0]*len(perc)\n",
    "\n",
    "for i, p in enumerate(perc):\n",
    "    sC[i] = smoothCurve(data, p, smo_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab810a",
   "metadata": {},
   "source": [
    "# 3 Determining N_m from the spline curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15a87d",
   "metadata": {},
   "source": [
    "Define the function to calculate N_m for a predefined precision (=U_N) based on the smooth spline curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_Nm(precision, ys):\n",
    "    \n",
    "    # if the required precision is already reached for less then 10 particles, return NaN\n",
    "    if ys[0] < precision:\n",
    "        #return 10  # return 10 instead of NaN when Nm is actually lower than 10  --> for Nm vs. IQR/d50 plot\n",
    "        return np.nan\n",
    "    \n",
    "    # otherwise calculate at which N the curve intersects the required value of the precision\n",
    "    else:\n",
    "        idx = np.argwhere(np.diff(np.sign(precision - ys))).flatten()\n",
    "        \n",
    "        if not idx.size > 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            idx = idx[0]\n",
    "            Nm_lin=xs[idx]\n",
    "            Nm=m.ceil(10**Nm_lin)\n",
    "            #print(Nm)\n",
    "            return Nm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac126d4",
   "metadata": {},
   "source": [
    "Define a set of U_N values for which you want to calculate N_m and initiate a dataframe to save the N_m values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01639fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "UN_out=[40, 30, 20, 15, 10, 8, 6, 5, 4, 3, 2, 1]\n",
    "df_Nm = pd.DataFrame()\n",
    "df_Nm['U_N']=UN_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af4a41",
   "metadata": {},
   "source": [
    "Apply the det_Nm function for each element of the U_N array and for each of the percentiles. Save the N_m dataframe to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f32ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(perc):\n",
    "    c = sC[i]\n",
    "    N=[]\n",
    "    for U in UN_out:\n",
    "        N.append(det_Nm(U, c))\n",
    "    print(N)\n",
    "    df_Nm[p]=N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ace51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Nm.to_csv('Nm_table_'+measurand+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d855982",
   "metadata": {},
   "source": [
    "# 4 Output plot(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f361e2",
   "metadata": {},
   "source": [
    "## 4.1 N_m vs U_N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3691e",
   "metadata": {},
   "source": [
    "Set the measurand for which you wanna produce the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb381925",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurand = 'Fmin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248606db",
   "metadata": {},
   "source": [
    "Read in the N_m data generated in section 3. If needed first go the right directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f266d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "#os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c84a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.read_csv('Nm_table_'+measurand+'.csv', header=[0], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a83b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a019a",
   "metadata": {},
   "source": [
    "Plot the N_m vs. U_N data on a log-log plot for each of the percentiles. You will see that a linear relation is obtained. Use the polyfit function to fit a first degree polynomial to the data. The relation can be used to estimate the amount of particles you should measure of your material to assure a certain precision of the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the columns names (=percentiles) of the plot_data dataframe \n",
    "key_list = list(plot_data)\n",
    "key_list.remove('U_N')\n",
    "\n",
    "# set the x values for the polyfit and define the colors to be used for the plotting of the different percentiles\n",
    "x_fit = np.arange(-1, 3)\n",
    "color = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# loop through the different percentiles and colors\n",
    "for k, c in zip(key_list, color):\n",
    "    # split the dataframe up in its columns and remove the NaN rows\n",
    "    df_sub = pd.DataFrame()\n",
    "    df_sub['U_N'] = plot_data['U_N']\n",
    "    df_sub['Nm'] = plot_data[k]\n",
    "    df_sub = df_sub.dropna()\n",
    "    \n",
    "    # plot the data points\n",
    "    plt.plot(df_sub['U_N'], df_sub['Nm'], ls='None', marker ='s', color=c, label=k)\n",
    "    \n",
    "    # perform the linear fit using polyfit\n",
    "    if len(df_sub)>1:\n",
    "        z = np.polyfit(np.log10(df_sub['U_N']), np.log10(df_sub['Nm']), 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(10.0**(x_fit), 10.0**(p(x_fit)), '-', color=c)\n",
    "        \n",
    "\n",
    "# plot settings        \n",
    "plt.xlabel(r'$U_{\\rm N}$ ($\\%$)', fontsize=20)\n",
    "plt.ylabel(r'$N_{\\rm m}$', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(1,40)\n",
    "plt.ylim(10,2*10**4)\n",
    "ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "ax.set_xticks([1,2,5,10, 20, 40])\n",
    "plt.legend(fontsize = 14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the figure\n",
    "plt.savefig('NumPar_vs_UN_'+measurand+'.png', dpi=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
